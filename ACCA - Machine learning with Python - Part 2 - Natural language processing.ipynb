{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://courses.edx.org/asset-v1:ACCA+ML001+2T2021+type@asset+block@acca-logo.jpg\" alt=\"ACCA logo\" style=\"width: 400px;\"/>\n",
    "\n",
    "# Machine learning with Python\n",
    "## Part 2 - Natural language processing\n",
    "\n",
    "* **Course:** __Machine learning with Python for finance professionals__ by ACCA\n",
    "* **Instructor:** [Coefficient](https://coefficient.ai) / [@CoefficientData](https://twitter.com/CoefficientData)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #BA001E; border: 0px; -moz-border-radius: 10px; -webkit-border-radius: 10px;\">\n",
    "<h2 style=\"color: white\">\n",
    "Text processing in scikit-learn\n",
    "</h2><br>\n",
    "</div>\n",
    "\n",
    "### Goal: Predict Category from item description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_excel(\"Grocery Database.xlsx\", sheet_name=\"Grosto DB\")\n",
    "orders.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's _just_ look at Category and Item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = orders[['Items', 'Category']].copy()\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset was purchase data, let's reduce from 50k purchases to 603 unique items only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates().reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the available categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Category.value_counts().sort_values(ascending=True).plot(kind='pie', figsize=(10,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples from each category\n",
    "df.groupby('Category').tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = {\n",
    "    # Bakery & Breakfast has 174 records, leave this alone\n",
    "    'Bakery & Breakfast': 'Bakery & Breakfast',\n",
    "    \n",
    "    # Fresh Food\n",
    "    'Fruit & Vegetable': 'Fresh Food',\n",
    "    'Dairy, Chilled & Eggs': 'Fresh Food',\n",
    "    'Meat & Seafood': 'Fresh Food',\n",
    "    \n",
    "    # Drinks\n",
    "    'Wines, Beers & Spirits': 'Drinks',\n",
    "    'Beverages': 'Drinks',\n",
    "    \n",
    "    # Cupboard\n",
    "    'Rice & Cooking Essentials': 'Cupboard',\n",
    "    'Choco, Snacks, Sweets': 'Cupboard',\n",
    "    'Health': 'Cupboard',\n",
    "    'Frozen': 'Cupboard',\n",
    "    \n",
    "    # Other\n",
    "    'Household': 'Other',\n",
    "    'Mother & Baby': 'Other',\n",
    "    'Beauty': 'Other',\n",
    "    'Pet Care': 'Other',\n",
    "    'Party Supplies': 'Other',\n",
    "    'Kitchen & Dining': 'Other',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target'] = df.Category.map(category_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of each of the new target categories?\n",
    "df.Target.value_counts().sort_values(ascending=True).plot(kind='pie', figsize=(5,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's ready for vectorization! Let's apply scikit-learn's CountVectorizer to the `Items` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a new vectorizer and specify how we want it to work. Think of this like constructing a machine to your specification, ready to feed all your text data into. ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=1000,     # max number of words to consider (uses first N most frequent)\n",
    "                             ngram_range=(1, 2),    # e.g. (1,1) for single words, (1,2) for bigrams, etc\n",
    "                             stop_words='english',  # remove English language stop words, e.g. 'to', 'the', 'it'\n",
    "                             binary=True)           # use 1/0 instead of word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `vectorizer` \"machine\" hasn't yet seen any of our data. Let's change that by feeding our item descriptions into the `.fit()` method. When this runs, it will \"learn a vocabulary\", i.e. what words appear in this data, and how often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(df.Items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning some vocabulary is only half the job. It's time for our `vectorizer` to apply what it learned and construct a \"document-term matrix\" containing one row for each sample and one column for each term (remember, a \"term\" may be 1 or even 2 consecutive words, as we specified a couple cells above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `transform` to generate the X \"word matrix\" - one column per feature (word or n-grams)\n",
    "vectorizer.transform(df.Items)\n",
    "# Sparse matrix! Only the non-zero entries are recorded..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sparse matrix has the same number of rows as our original data (603) and 500 columns (because we specified `max_features=500` earlier when creating our `vectorizer`). However, it's stored in a compressed format that might be machine-friendly but isn't human-friendly. Let's fix that, first by converting it to a NumPy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call .toarray() to transform this into a full matrix (less space optimised)\n",
    "vectorizer.transform(df.Items).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here? Most entries are actually zero (this is what \"sparse matrix\" means, the matrix is mostly empty). This is because most rows don't contain that many words, so for the top 1k words most of them won't be in a short description like \"Oreo mini oreo sharepack\".\n",
    "\n",
    "The data is all here, but it's still not friendly as we're missing our column names (i.e. the word terms themselves). These have been saved for us into `vectorizer.get_feature_names()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform our vectorizer ‚û° turn it into a NumPy matrix ‚û° add in the feature names ‚û° store all this in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call .toarray() to transform this into a full matrix (less space optimised)\n",
    "X = pd.DataFrame(vectorizer.transform(df.Items).toarray(),\n",
    "                 columns=vectorizer.get_feature_names())\n",
    "y = df.Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all columns are shown - you can disable this by calling pd.set_option('display.max_columns', None)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the non-zero entries in the first row, and compare to the original text.\n",
    "\n",
    "# Original text\n",
    "print(df.Items[1])\n",
    "\n",
    "# Add some blank lines\n",
    "print('\\n')\n",
    "\n",
    "# Non-zero entries in the first row\n",
    "first_row = X.loc[1]\n",
    "print(first_row[first_row > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #BA001E; border: 0px; -moz-border-radius: 10px; -webkit-border-radius: 10px;\">\n",
    "<h2 style=\"color: white\">\n",
    "Build a random-forest text classifier\n",
    "</h2><br>\n",
    "</div>\n",
    "\n",
    "This code should be very familiar from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble.RandomForestClassifier(n_estimators=20)\n",
    "model_selection.cross_val_score(model, X, y, scoring='accuracy', cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What features are most important?\n",
    "model.fit(X, y)\n",
    "feature_importances = pd.DataFrame({'Features' : X.columns, 'Importance Score': model.feature_importances_})\n",
    "feature_importances = feature_importances.sort_values('Importance Score', ascending=False)\n",
    "feature_importances.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's also calculate which way these keywords influence the categorisation decision\n",
    "\n",
    "def is_word_in_text(x, word):\n",
    "    return word in x\n",
    "\n",
    "def percentify(x):\n",
    "    \"\"\"Turns 0.1234 into 12%\"\"\"\n",
    "    return f\"{100*x:.0f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in feature_importances.Features.head(10):\n",
    "    print('\\n\\n-------------------\\n\\n')\n",
    "    df[word] = df.Items.apply(is_word_in_text, word=word)\n",
    "    print(\"Word:\", word)\n",
    "\n",
    "    # We want to calculate which categories contain this term,\n",
    "    # and also what % of the items in the category contains the term\n",
    "    percent_that_contains_word = df.groupby('Target')[word].mean()\n",
    "    percent_that_contains_word\n",
    "\n",
    "    # Most words are only in 1-2 categories, so let's ignore\n",
    "    # categories which don't contain the word\n",
    "    percent_that_contains_word = (\n",
    "        percent_that_contains_word\n",
    "        .sort_values(ascending=False)\n",
    "        .reset_index()\n",
    "        .query(f\"{word} > 0\")\n",
    "    )\n",
    "\n",
    "    percent_that_contains_word\n",
    "\n",
    "    # Convert 0.212644 into 21.3%\n",
    "    percent_that_contains_word[word] = percent_that_contains_word[word].apply(percentify)\n",
    "\n",
    "    print(percent_that_contains_word)\n",
    "\n",
    "# How to read this?\n",
    "\n",
    "#     Word: bread\n",
    "#                  Category     bread\n",
    "#     0  Bakery & Breakfast     21%\n",
    "\n",
    "# \"bread\" is the first item in this list, because \"bread\" is the feature\n",
    "# with the highest importance score according to the RandomForestClassifier.\n",
    "\n",
    "# The above suggests that this is true because \"bread\" appears in 21% of Bakery & Breakfast rows,\n",
    "# and in no other categories. This is a fairly strong signal that rows containing \"bread\" belong\n",
    "# to the \"Bakery & Breakfast\" category!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b><i class=\"fa fa-check-square\" aria-hidden=\"true\"></i>&nbsp; Check</b><br>\n",
    "\n",
    "There's a lot going on in the last few cells. Run through them line by line, and ensure you follow along. If there are any pandas functions that you're unclear on it's **strongly** encouraged you take a moment to double check the [pandas cheat sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) or the pandas documentation or otherwise review the previous course materials.\n",
    "    \n",
    "You should also check if you fully understand what's happening in this last cell? We explained the bread example. Here's another example for \"oats\" (this may be the final example above, although random forests _are_ random so it may not be!):\n",
    "\n",
    "```\n",
    "Word: oats\n",
    "               Target oats\n",
    "0  Bakery & Breakfast   7%\n",
    "1              Drinks   1%\n",
    "```\n",
    "    \n",
    "The above suggests that \"oats\" is an important feature because:\n",
    "    - it appears in 7% of `Bakery & Breakfast`\n",
    "    - it appears in 1% of `Drinks`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples containing \"oats\"...could be breakfast oats or oat milk\n",
    "df[X['oats'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> ### üö© Exercise\n",
    "> Copy your hyperparameter tuning code from the previous notebook, and identify the best `max_depth` and `n_estimators` for a `RandomForestClassifier` for this problem.\n",
    "> \n",
    "> **Tips:**\n",
    "> - You don't need to change much at all! This is why we use generic variables like `df` and `X` and `y` and `model`...it means code you write to solve one problem is abstract enough that it can be copied verbatim to solve another problem. This is a **huge** productivity win, as you'll find out now.\n",
    "> - You are welcome to try adjusting the other hyperparameters for a **[RandomForestClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)**, it's good practice! We encourage you to read scikit-learn's **[advice on tuning random forest parameters](https://scikit-learn.org/stable/modules/ensemble.html#random-forest-parameters)** as it's a great example of _why_ this is such a high quality library. This advice are the result of the library's authors distilling countless research papers into best practices!\n",
    "> - _However_, if you do try experimenting with other hyperparameters, don't spend too long. `max_depth` and `n_estimators` are the big ones, and you'll get a lot more \"bang for your buck\" by focusing on adding some more features first and coming back to model tuning at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è ENTER YOUR SOLUTION HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #BA001E; border: 0px; -moz-border-radius: 10px; -webkit-border-radius: 10px;\">\n",
    "<h2 style=\"color: white\">\n",
    "Adding more features\n",
    "</h2><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> ### üö© Exercise\n",
    "> Take your best settings for `max_depth` and `n_estimators` and enter them into the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è ENTER YOUR SOLUTION HERE\n",
    "\n",
    "best_max_depth = 1\n",
    "best_n_estimators = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble.RandomForestClassifier(n_estimators=best_n_estimators, max_depth=best_max_depth)\n",
    "model_selection.cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can add more features from our original dataset `orders` to our input feature matrix `X` (which contains only vectorizer-generated words/terms at the moment) as follows. Let's review the three dataframes in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders - this is our original dataframe with 50k rows and 32 cols\n",
    "print(orders.shape)\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df - contains one row for each product + item descriptions + categories + columns added in\n",
    "#      our \"what % of category X contains word Y\" feature interpretation step earlier\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X - this is our feature matrix that we input into the ML model\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orders has some useful info that we can add to X. Let's add Price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a lookup from Items to Price\n",
    "items_to_price = (\n",
    "    orders[['Items', 'Price']]\n",
    "    .drop_duplicates(subset='Items')\n",
    "    .set_index('Items')['Price']\n",
    ")\n",
    "items_to_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this lookup to add it into df\n",
    "df['Price'] = df.Items.map(items_to_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df maps 1:1 to X (they have the same number of rows) so we can just copy it across\n",
    "X['Price'] = df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuitively, should adding Price help?\n",
    "plt.figure(figsize=(25, 6))\n",
    "sns.boxplot(x='Target', y='Price', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does including Price help?\n",
    "model = ensemble.RandomForestClassifier(n_estimators=best_n_estimators, max_depth=best_max_depth)\n",
    "\n",
    "print(\n",
    "    'Without Price:',\n",
    "    model_selection.cross_val_score(model, X.drop(columns='Price'), y, cv=5, scoring='accuracy').mean()\n",
    ")\n",
    "\n",
    "print(\n",
    "    'With Price:',\n",
    "    model_selection.cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b><i class=\"fa fa-check-square\" aria-hidden=\"true\"></i>&nbsp; Check</b><br>\n",
    "\n",
    "Did adding Price improve this model noticeably? Try re-running the cell above a few times to get a sense of how much is \"random variation\" (due to the random shuffling in k-fold cross-validation or the random forest itself) and how much is a real difference, if any.\n",
    "    \n",
    "Were you expecting this to be a useful feature?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #BA001E; border: 0px; -moz-border-radius: 10px; -webkit-border-radius: 10px;\">\n",
    "<h2 style=\"color: white\">\n",
    "Model architecture selection\n",
    "</h2><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we included this graphic, taken from [this page on the scikit-learn documentation](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).\n",
    "\n",
    "If we follow the prompts, it suggests we try a Naive Bayes approach for this type of problem. Naive Bayes methods are a great technique to consider when building text classifiers and their history dates back to the very first email spam detection algorithms. We won't go into detail here on what Naive Bayes methods are or how they work, but do feel free to read scikit-learn's [excellent user guide on Naive Bayes techniques](https://scikit-learn.org/stable/modules/naive_bayes.html) or listen to this friendly [short Data Skeptic podcast episode on Naive Bayes classifiers for spam detection](https://dataskeptic.com/blog/episodes/2018/spam-filtering-with-naive-bayes).\n",
    "\n",
    "<img src=\"https://courses.edx.org/asset-v1:ACCA+ML001+2T2021+type@asset+block@ml_map.png\" alt=\"ML map\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> ### üö© Exercise\n",
    "> We've given you a Naive Bayes model in the cell below. Find & copy in your line of code from earlier that calculates the five-fold cross-validated accuracy, given `X`, `y` and a model. It _should_ work directly with the `BernoulliNB` below without any issues. It should also provide an accuracy improvement _far_ better than anything else we've done so far!\n",
    "> \n",
    "> This consistency is one of the best features of scikit-learn's design: whether you're working with linear methods, decision trees, random forests, support vector machines, neural networks...it doesn't matter, everything is just \"plug-and-play\". Want to try out an entirely different model architecture? Just swap out the one you have for a new one, easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns='Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did we know to set alpha=0.1? We may have done some hyperparameter tuning in advance,\n",
    "# feel free to replicate and confirm our findings.\n",
    "model = naive_bayes.BernoulliNB(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è ENTER YOUR SOLUTION HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = naive_bayes.BernoulliNB(alpha=0.1).fit(X, y)\n",
    "df['Predicted'] = model.predict(X)\n",
    "df['Correct'] = df.Predicted == y\n",
    "df.query(\"Correct == False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #BA001E; border: 0px; -moz-border-radius: 10px; -webkit-border-radius: 10px;\">\n",
    "<h2 style=\"color: white\">\n",
    "Predicting for new text input\n",
    "</h2><br>\n",
    "</div>\n",
    "\n",
    "We're going to return to the simple text-only classifier and train a BernoulliNB model to learn to predict the target categories. Everything we've done so far is simply model evaluation to find the best \"recipe\".\n",
    "\n",
    "Now we know the best recipe (`BernoulliNB(alpha=0.1)`) it's time to use that recipe to prepare our competition-winning cake. In other words, let's train the model on the full dataset and take it for a spin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create X to work with just text data, and then train a model on the full dataset\n",
    "X = pd.DataFrame(vectorizer.transform(df.Items).toarray(),\n",
    "                 columns=vectorizer.get_feature_names())\n",
    "y = df.Target\n",
    "\n",
    "model = naive_bayes.BernoulliNB(alpha=0.1).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's construct some new data, I've made these up!\n",
    "new_text = pd.Series([\n",
    "    # Bakery & Breakfast\n",
    "    'honey & maple syrup porridge',\n",
    "    'bran flakes',\n",
    "    \n",
    "    # Cupboard\n",
    "    'arborio risotto rice',\n",
    "    'baking powder',\n",
    "\n",
    "    # Drinks\n",
    "    'cabernet sauvignon red wine',\n",
    "    'sparkling water',\n",
    "    \n",
    "    # Fresh Food\n",
    "    'wheel of cheese',\n",
    "    'mixed grapes',\n",
    "    \n",
    "    # Other\n",
    "    'cat food',\n",
    "    'washing up liquid',\n",
    "    \n",
    "    # Trickier edge cases\n",
    "    'banana bread',  # is it bananas or bread?\n",
    "    'grape juice',\n",
    "    'chocolate orange',  # one of your 5-a-day?\n",
    "    \n",
    "    # ADD YOUR OWN EXAMPLES UNDER HERE\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to vectorize the data first, but using the EXACT SAME vectorizer\n",
    "# (remember the vectorizer was fitted to the training data)\n",
    "X_predict = pd.DataFrame(vectorizer.transform(new_text).toarray(),\n",
    "                         columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This what the classifier model actually sees\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only words in the feature matrix that are in our new_text list\n",
    "non_zero_cols = (X_predict.sum() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick out the columns that match this filter\n",
    "non_zero_col_names = X_predict.columns[non_zero_cols]\n",
    "non_zero_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the filtered dataframe and add in new_text as the index so it's easy to review\n",
    "new_text_feature_matrix = X_predict[non_zero_col_names].set_index(new_text)\n",
    "new_text_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a great way to visualise this matrix\n",
    "sns.heatmap(new_text_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the model's predictions\n",
    "model.predict(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dataframe showing the predicted class + probabilities for other classes\n",
    "# This gives us insight into the model's confidence for each prediction\n",
    "\n",
    "predicted_classes = pd.DataFrame(model.predict(X_predict), columns=['Prediction'])\n",
    "\n",
    "predicted_probabilities = pd.DataFrame(model.predict_proba(X_predict), columns=model.classes_)\n",
    "predicted_probabilities = (predicted_probabilities * 100).round()  # formatted as %\n",
    "\n",
    "# Horizontally concatenate & add in new_text\n",
    "predictions = pd.concat(\n",
    "    [predicted_classes,\n",
    "     predicted_probabilities], axis=1).set_index(new_text)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display stacked bar chart showing the predicted class probabilities for each item\n",
    "predictions.iloc[::-1].plot.barh(stacked=True, figsize=(10,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
